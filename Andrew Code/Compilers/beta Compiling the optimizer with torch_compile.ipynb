{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94f17215-3fe6-4c67-a940-de65e16cf968",
   "metadata": {},
   "source": [
    "# (beta) Compiling the optimizer with torch.compile\n",
    "\n",
    "The optimizer is a key algorithm for training any deep learning model. Since it is responsible for updating every model parameter, it can often become the bottleneck in training performance for large models. In this recipe, we will apply torch.compile to the optimizer to observe the GPU performance improvement.\n",
    "\n",
    "Note\n",
    "\n",
    "This tutorial requires PyTorch 2.2.0 or later.\n",
    "\n",
    "## Model Setup\n",
    "For this example, we’ll use a simple sequence of linear layers. Since we are only benchmarking the optimizer, the choice of model doesn’t matter because optimizer performance is a function of the number of parameters.\n",
    "\n",
    "Depending on what machine you are using, your exact results may vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "183292a8-7d88-411b-bd25-5d3880ce9da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#switch cuda for mps since on mac\n",
    "model = torch.nn.Sequential(\n",
    "    *[torch.nn.Linear(1024, 1024, False, device=\"mps\") for _ in range(10)]\n",
    ")\n",
    "input = torch.rand(1024, device=\"mps\")\n",
    "output = model(input)\n",
    "output.sum().backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae28e14-a9c5-4610-9be1-f4c839998304",
   "metadata": {},
   "source": [
    "## Setting up and running the optimizer benchmark\n",
    "In this example, we’ll use the Adam optimizer and create a helper function to wrap the step() in torch.compile().\n",
    "\n",
    "Note\n",
    "\n",
    "torch.compile is only supported on cuda devices with compute capability >= 7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33b22300-ef66-4331-a18b-f6853cc70837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MPS is available and can be used.\n",
      "eager runtime: 5314.8826731270865us\n",
      "compiled runtime: 54.81313662917427us\n"
     ]
    }
   ],
   "source": [
    "# exit cleanly if we are on a device that doesn't support torch.compile\n",
    "#if torch.cuda.get_device_capability() < (7, 0):\n",
    "#    print(\"Exiting because torch.compile is not supported on this device.\")\n",
    "#    import sys\n",
    "#    sys.exit(0)\n",
    "\n",
    "\n",
    "#since on mac and no Cuda, use this\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"✅ MPS is available and can be used.\")\n",
    "else:\n",
    "    print(\"❌ MPS is not available.\")\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "@torch.compile(fullgraph=False)\n",
    "def fn():\n",
    "    opt.step()\n",
    "\n",
    "\n",
    "# Let's define a helpful benchmarking function:\n",
    "import torch.utils.benchmark as benchmark\n",
    "\n",
    "\n",
    "def benchmark_torch_function_in_microseconds(f, *args, **kwargs):\n",
    "    t0 = benchmark.Timer(\n",
    "        stmt=\"f(*args, **kwargs)\", globals={\"args\": args, \"kwargs\": kwargs, \"f\": f}\n",
    "    )\n",
    "    return t0.blocked_autorange().mean * 1e6\n",
    "\n",
    "\n",
    "# Warmup runs to compile the function\n",
    "for _ in range(5):\n",
    "    fn()\n",
    "\n",
    "eager_runtime = benchmark_torch_function_in_microseconds(opt.step)\n",
    "compiled_runtime = benchmark_torch_function_in_microseconds(fn)\n",
    "\n",
    "assert eager_runtime > compiled_runtime\n",
    "\n",
    "print(f\"eager runtime: {eager_runtime}us\")\n",
    "print(f\"compiled runtime: {compiled_runtime}us\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ee8721-de77-4409-95cc-d6d99b27644d",
   "metadata": {},
   "source": [
    "my test to see how much faster mps is here than cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "067a30a6-9915-4996-8b1f-51f769badfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    *[torch.nn.Linear(1024, 1024, False, device=\"cpu\") for _ in range(10)]\n",
    ")\n",
    "input = torch.rand(1024, device=\"cpu\")\n",
    "output = model(input)\n",
    "output.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5f6adef-6564-4fe0-b4e6-378923bba89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eager runtime: 12911.591009469703us\n",
      "compiled runtime: 8900.201069596022us\n"
     ]
    }
   ],
   "source": [
    "opt = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "@torch.compile(fullgraph=False)\n",
    "def fn():\n",
    "    opt.step()\n",
    "\n",
    "\n",
    "# Let's define a helpful benchmarking function:\n",
    "import torch.utils.benchmark as benchmark\n",
    "\n",
    "\n",
    "def benchmark_torch_function_in_microseconds(f, *args, **kwargs):\n",
    "    t0 = benchmark.Timer(\n",
    "        stmt=\"f(*args, **kwargs)\", globals={\"args\": args, \"kwargs\": kwargs, \"f\": f}\n",
    "    )\n",
    "    return t0.blocked_autorange().mean * 1e6\n",
    "\n",
    "\n",
    "# Warmup runs to compile the function\n",
    "for _ in range(5):\n",
    "    fn()\n",
    "\n",
    "eager_runtime = benchmark_torch_function_in_microseconds(opt.step)\n",
    "compiled_runtime = benchmark_torch_function_in_microseconds(fn)\n",
    "\n",
    "assert eager_runtime > compiled_runtime\n",
    "\n",
    "print(f\"eager runtime: {eager_runtime}us\")\n",
    "print(f\"compiled runtime: {compiled_runtime}us\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27aae61a-e256-40c8-90d3-312354159834",
   "metadata": {},
   "source": [
    "mps if faster here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d8b9d3-b862-4d9a-8d61-11fec1b4c327",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
