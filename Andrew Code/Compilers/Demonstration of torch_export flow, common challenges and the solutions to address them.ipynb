{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b335f15d-b0c3-486b-80cc-80b4a52c0f14",
   "metadata": {},
   "source": [
    "# Demonstration of torch.export flow, common challenges and the solutions to address them\n",
    "\n",
    "n the Introduction to torch.export Tutorial , we learned how to use torch.export. This tutorial expands on the previous one and explores the process of exporting popular models with code, as well as addresses common challenges that may arise with torch.export.\n",
    "\n",
    "In this tutorial, you will learn how to export models for these use cases:\n",
    "\n",
    "Video classifier (MViT)\n",
    "\n",
    "Automatic Speech Recognition (OpenAI Whisper-Tiny)\n",
    "\n",
    "Image Captioning (BLIP)\n",
    "\n",
    "Promptable Image Segmentation (SAM2)\n",
    "\n",
    "## Key requirement for torch.export: No graph break\n",
    "\n",
    "Each of the four models were chosen to demonstrate unique features of torch.export, as well as some practical considerations and issues faced in the implementation.\n",
    "\n",
    "torch.compile speeds up PyTorch code by using JIT to compile PyTorch code into optimized kernels. It optimizes the given model using TorchDynamo and creates an optimized graph , which is then lowered into the hardware using the backend specified in the API. When TorchDynamo encounters unsupported Python features, it breaks the computation graph, lets the default Python interpreter handle the unsupported code, and then resumes capturing the graph. This break in the computation graph is called a graph break.\n",
    "\n",
    "One of the key differences between torch.export and torch.compile is that torch.export doesnâ€™t support graph breaks which means that the entire model or part of the model that you are exporting needs to be a single graph. This is because handling graph breaks involves interpreting the unsupported operation with default Python evaluation, which is incompatible with what torch.export is designed for. You can read details about the differences between the various PyTorch frameworks in this link\n",
    "\n",
    "You can identify graph breaks in your program by using the following command:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64dc0af3-d861-4027-8e93-2c21f7598a4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1138833617.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mTORCH_LOGS=\"graph_breaks\" python <file_name>.py\u001b[39m\n                              ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "TORCH_LOGS=\"graph_breaks\" python <file_name>.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a683fdf1-c60d-49e0-bf0b-2a1fdd1cb974",
   "metadata": {},
   "source": [
    "You will need to modify your program to get rid of graph breaks. Once resolved, you are ready to export the model. PyTorch runs nightly benchmarks for torch.compile on popular HuggingFace and TIMM models. Most of these models have no graph breaks.\n",
    "\n",
    "The models in this recipe have no graph breaks, but fail with torch.export.\n",
    "\n",
    "# Video Classification\n",
    "MViT is a class of models based on MultiScale Vision Transformers. This model has been trained for video classification using the Kinetics-400 Dataset. This model with a relevant dataset can be used for action recognition in the context of gaming.\n",
    "\n",
    "The code below exports MViT by tracing with batch_size=2 and then checks if the ExportedProgram can run with batch_size=4.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7c8e2e6-d620-4252-90a5-03811ffbfcc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/p_/7g5qyc8d5zv5nfywm65yhpkc0000gn/T/ipykernel_64637/159176910.py\", line 23, in <module>\n",
      "    exported_program.module()(input_frames)\n",
      "  File \"/opt/anaconda3/envs/torch/lib/python3.12/site-packages/torch/fx/graph_module.py\", line 848, in call_wrapped\n",
      "    return self._wrapped_call(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/torch/lib/python3.12/site-packages/torch/fx/graph_module.py\", line 424, in __call__\n",
      "    raise e\n",
      "  File \"/opt/anaconda3/envs/torch/lib/python3.12/site-packages/torch/fx/graph_module.py\", line 411, in __call__\n",
      "    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1879, in _call_impl\n",
      "    return inner()\n",
      "           ^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1806, in inner\n",
      "    args_kwargs_result = hook(self, args, kwargs)  # type: ignore[misc]\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/torch/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py\", line 929, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/torch/lib/python3.12/site-packages/torch/export/_unlift.py\", line 83, in _check_input_constraints_pre_hook\n",
      "    _check_input_constraints_for_graph(\n",
      "  File \"/opt/anaconda3/envs/torch/lib/python3.12/site-packages/torch/_export/utils.py\", line 426, in _check_input_constraints_for_graph\n",
      "    _check_symint(\n",
      "  File \"/opt/anaconda3/envs/torch/lib/python3.12/site-packages/torch/_export/utils.py\", line 390, in _check_symint\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Expected input at *args[0].shape[0] to be equal to 2, but got 4. If you meant for this dimension to be dynamic, please re-export and specify dynamic_shapes (e.g. with Dim.DYNAMIC)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.models.video import MViT_V1_B_Weights, mvit_v1_b\n",
    "import traceback as tb\n",
    "\n",
    "model = mvit_v1_b(weights=MViT_V1_B_Weights.DEFAULT)\n",
    "\n",
    "# Create a batch of 2 videos, each with 16 frames of shape 224x224x3.\n",
    "input_frames = torch.randn(2,16, 224, 224, 3)\n",
    "# Transpose to get [1, 3, num_clips, height, width].\n",
    "input_frames = np.transpose(input_frames, (0, 4, 1, 2, 3))\n",
    "\n",
    "# Export the model.\n",
    "exported_program = torch.export.export(\n",
    "    model,\n",
    "    (input_frames,),\n",
    ")\n",
    "\n",
    "# Create a batch of 4 videos, each with 16 frames of shape 224x224x3.\n",
    "input_frames = torch.randn(4,16, 224, 224, 3)\n",
    "input_frames = np.transpose(input_frames, (0, 4, 1, 2, 3))\n",
    "try:\n",
    "    exported_program.module()(input_frames)\n",
    "except Exception:\n",
    "    tb.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdb7f5a-7b3a-42d3-977c-ab63e27d1fea",
   "metadata": {},
   "source": [
    "\n",
    "## Error: Static batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "156dac32-95bf-42ea-abac-9644b1863e85",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2460721820.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mraise RuntimeError(\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "    raise RuntimeError(\n",
    "RuntimeError: Expected input at *args[0].shape[0] to be equal to 2, but got 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a521f96d-5f4e-414a-9cbc-9e415fc1e161",
   "metadata": {},
   "source": [
    "By default, the exporting flow will trace the program assuming that all input shapes are static, so if you run the program with input shapes that are different than the ones you used while tracing, you will run into an error.\n",
    "\n",
    "## Solution\n",
    "To address the error, we specify the first dimension of the input (batch_size) to be dynamic , specifying the expected range of batch_size. In the corrected example shown below, we specify that the expected batch_size can range from 1 to 16. One detail to notice that min=2 is not a bug and is explained in The 0/1 Specialization Problem. A detailed description of dynamic shapes for torch.export can be found in the export tutorial. The code shown below demonstrates how to export mViT with dynamic batch sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cdb7c0-1cbf-452d-a915-d9fb2748fab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.models.video import MViT_V1_B_Weights, mvit_v1_b\n",
    "import traceback as tb\n",
    "\n",
    "\n",
    "model = mvit_v1_b(weights=MViT_V1_B_Weights.DEFAULT)\n",
    "\n",
    "# Create a batch of 2 videos, each with 16 frames of shape 224x224x3.\n",
    "input_frames = torch.randn(2,16, 224, 224, 3)\n",
    "\n",
    "# Transpose to get [1, 3, num_clips, height, width].\n",
    "input_frames = np.transpose(input_frames, (0, 4, 1, 2, 3))\n",
    "\n",
    "# Export the model.\n",
    "batch_dim = torch.export.Dim(\"batch\", min=2, max=16)\n",
    "exported_program = torch.export.export(\n",
    "    model,\n",
    "    (input_frames,),\n",
    "    # Specify the first dimension of the input x as dynamic\n",
    "    dynamic_shapes={\"x\": {0: batch_dim}},\n",
    ")\n",
    "\n",
    "# Create a batch of 4 videos, each with 16 frames of shape 224x224x3.\n",
    "input_frames = torch.randn(4,16, 224, 224, 3)\n",
    "input_frames = np.transpose(input_frames, (0, 4, 1, 2, 3))\n",
    "try:\n",
    "    exported_program.module()(input_frames)\n",
    "except Exception:\n",
    "    tb.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949e629e-9a0e-421c-96ac-6b52ac17caa8",
   "metadata": {},
   "source": [
    "## Automatic Speech Recognition\n",
    "Automatic Speech Recognition (ASR) is the use of machine learning to transcribe spoken language into text. Whisper is a Transformer based encoder-decoder model from OpenAI, which was trained on 680k hours of labelled data for ASR and speech translation. The code below tries to export whisper-tiny model for ASR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e712756e-482a-41de-9428-003c9570d562",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b58ee542-dc50-4cb2-bc05-d7083451ecf7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected `mod` to be an instance of `torch.nn.Module`, got <class 'function'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs.logits \n\u001b[32m     22\u001b[39m model.eval()\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m exported_program: torch.export.ExportedProgram= \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforward_for_export\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m                                                                    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/torch/lib/python3.12/site-packages/torch/export/__init__.py:275\u001b[39m, in \u001b[36mexport\u001b[39m\u001b[34m(mod, args, kwargs, dynamic_shapes, strict, preserve_module_call_signature)\u001b[39m\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_trace\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _export\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, torch.nn.Module):\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    276\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected `mod` to be an instance of `torch.nn.Module`, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(mod)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    277\u001b[39m     )\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, torch.jit.ScriptModule):\n\u001b[32m    279\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    280\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mExporting a ScriptModule is not supported. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    281\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMaybe try converting your ScriptModule to an ExportedProgram \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    282\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33musing `TS2EPConverter(mod, args, kwargs).convert()` instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    283\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Expected `mod` to be an instance of `torch.nn.Module`, got <class 'function'>."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "\n",
    "# load model\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n",
    "\n",
    "# dummy inputs for exporting the model\n",
    "input_features = torch.randn(1,80, 3000)\n",
    "attention_mask = torch.ones(1, 3000)\n",
    "decoder_input_ids = torch.tensor([[1, 1, 1 , 1]]) * model.config.decoder_start_token_id\n",
    "\n",
    "def forward_for_export(input_features, attention_mask, decoder_input_ids):\n",
    "    outputs = model(\n",
    "        input_features,\n",
    "        attention_mask=attention_mask,\n",
    "        decoder_input_ids=decoder_input_ids,\n",
    "        use_cache=False \n",
    "    )\n",
    "    return outputs.logits \n",
    "\n",
    "model.eval()\n",
    "\n",
    "exported_program: torch.export.ExportedProgram= torch.export.export(forward_for_export, \n",
    "                                                                    args=(input_features, attention_mask, decoder_input_ids,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6868d3e3-8f71-49e5-8635-c9b93f08f256",
   "metadata": {},
   "source": [
    "### error:\n",
    "\n",
    "torch._dynamo.exc.InternalTorchDynamoError: AttributeError: 'DynamicCache' object has no attribute 'key_cache'\n",
    "\n",
    "\n",
    "By default torch.export traces your code using TorchDynamo, a byte-code analysis engine, which symbolically analyzes your code and builds a graph. This analysis provides a stronger guarantee about safety but not all Python code is supported. When we export the whisper-tiny model using the default strict mode, it typically returns an error in Dynamo due to an unsupported feature. To understand why this errors in Dynamo, you can refer to this GitHub issue.\n",
    "\n",
    "### Solution\n",
    "To address the above error , torch.export supports the non_strict mode where the program is traced using the Python interpreter, which works similar to PyTorch eager execution. The only difference is that all Tensor objects will be replaced by ProxyTensors, which will record all their operations into a graph. By using strict=False, we are able to export the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90c8a750-8c63-486f-b179-07979bcb637b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found <class 'transformers.cache_utils.EncoderDecoderCache'> in output, which is not a known type. If this type holds tensors, you need to register a pytree for it. See https://github.com/pytorch/functorch/issues/475 for a brief explanation why. If you don't need to register a pytree, please leave a comment explaining your use case and we'll make this more ergonomic to deal with",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     11\u001b[39m decoder_input_ids = torch.tensor([[\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m , \u001b[32m1\u001b[39m]]) * model.config.decoder_start_token_id\n\u001b[32m     13\u001b[39m model.eval()\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m exported_program: torch.export.ExportedProgram= \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m                                                                    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/torch/lib/python3.12/site-packages/torch/export/__init__.py:319\u001b[39m, in \u001b[36mexport\u001b[39m\u001b[34m(mod, args, kwargs, dynamic_shapes, strict, preserve_module_call_signature)\u001b[39m\n\u001b[32m    317\u001b[39m     new_msg = \u001b[38;5;28mstr\u001b[39m(e) + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + draft_export_msg\n\u001b[32m    318\u001b[39m     e.args = (new_msg,)\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/torch/lib/python3.12/site-packages/torch/export/__init__.py:286\u001b[39m, in \u001b[36mexport\u001b[39m\u001b[34m(mod, args, kwargs, dynamic_shapes, strict, preserve_module_call_signature)\u001b[39m\n\u001b[32m    279\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    280\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mExporting a ScriptModule is not supported. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    281\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMaybe try converting your ScriptModule to an ExportedProgram \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    282\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33musing `TS2EPConverter(mod, args, kwargs).convert()` instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    283\u001b[39m     )\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdynamic_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    296\u001b[39m     draft_export_msg = (\n\u001b[32m    297\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe error above occurred when calling torch.export.export. If you would \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    298\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlike to view some more information about this error, and get a list \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    299\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mof all other errors that may occur in your export call, you can \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    300\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mreplace your `export()` call with `draft_export()`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    301\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/torch/lib/python3.12/site-packages/torch/export/_trace.py:1164\u001b[39m, in \u001b[36m_log_export_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1158\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mpartial_fx_graph\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1159\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m   1160\u001b[39m             e.partial_fx_graph,\n\u001b[32m   1161\u001b[39m             file=sys.stderr,\n\u001b[32m   1162\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1164\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1166\u001b[39m     _EXPORT_FLAGS = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/torch/lib/python3.12/site-packages/torch/export/_trace.py:1130\u001b[39m, in \u001b[36m_log_export_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1128\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1129\u001b[39m     start = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m1130\u001b[39m     ep = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1131\u001b[39m     end = time.time()\n\u001b[32m   1132\u001b[39m     log_export_usage(\n\u001b[32m   1133\u001b[39m         event=\u001b[33m\"\u001b[39m\u001b[33mexport.time\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1134\u001b[39m         metrics=end - start,\n\u001b[32m   1135\u001b[39m         flags=_EXPORT_FLAGS,\n\u001b[32m   1136\u001b[39m         **get_ep_stats(ep),\n\u001b[32m   1137\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/torch/lib/python3.12/site-packages/torch/export/exported_program.py:123\u001b[39m, in \u001b[36m_disable_prexisiting_fake_mode.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(fn)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    122\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m unset_fake_temporarily():\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/torch/lib/python3.12/site-packages/torch/export/_trace.py:2176\u001b[39m, in \u001b[36m_export\u001b[39m\u001b[34m(mod, args, kwargs, dynamic_shapes, strict, preserve_module_call_signature, pre_dispatch, allow_complex_guards_as_runtime_asserts, _is_torch_jit_trace)\u001b[39m\n\u001b[32m   2168\u001b[39m \u001b[38;5;66;03m# NOTE Export training IR rollout\u001b[39;00m\n\u001b[32m   2169\u001b[39m \u001b[38;5;66;03m# Old export calls export._trace(pre_dispatch=True)\u001b[39;00m\n\u001b[32m   2170\u001b[39m \u001b[38;5;66;03m# and there are still lot of internal/OSS callsites that\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2173\u001b[39m \u001b[38;5;66;03m# export_training_ir_rollout_check returns True in OSS\u001b[39;00m\n\u001b[32m   2174\u001b[39m \u001b[38;5;66;03m# while internally it returns False UNLESS otherwise specified.\u001b[39;00m\n\u001b[32m   2175\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;129;01mand\u001b[39;00m export_training_ir_rollout_check():\n\u001b[32m-> \u001b[39m\u001b[32m2176\u001b[39m     ep = \u001b[43m_export_for_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2177\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2178\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2179\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2180\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdynamic_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2181\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2182\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2183\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2184\u001b[39m     dtrace_structured(\u001b[33m\"\u001b[39m\u001b[33mexported_program\u001b[39m\u001b[33m\"\u001b[39m, payload_fn=\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mstr\u001b[39m(ep))\n\u001b[32m   2185\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ep\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/torch/lib/python3.12/site-packages/torch/export/_trace.py:1164\u001b[39m, in \u001b[36m_log_export_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1158\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mpartial_fx_graph\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1159\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m   1160\u001b[39m             e.partial_fx_graph,\n\u001b[32m   1161\u001b[39m             file=sys.stderr,\n\u001b[32m   1162\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1164\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1166\u001b[39m     _EXPORT_FLAGS = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/torch/lib/python3.12/site-packages/torch/export/_trace.py:1130\u001b[39m, in \u001b[36m_log_export_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1128\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1129\u001b[39m     start = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m1130\u001b[39m     ep = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1131\u001b[39m     end = time.time()\n\u001b[32m   1132\u001b[39m     log_export_usage(\n\u001b[32m   1133\u001b[39m         event=\u001b[33m\"\u001b[39m\u001b[33mexport.time\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1134\u001b[39m         metrics=end - start,\n\u001b[32m   1135\u001b[39m         flags=_EXPORT_FLAGS,\n\u001b[32m   1136\u001b[39m         **get_ep_stats(ep),\n\u001b[32m   1137\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/torch/lib/python3.12/site-packages/torch/export/exported_program.py:123\u001b[39m, in \u001b[36m_disable_prexisiting_fake_mode.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(fn)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    122\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m unset_fake_temporarily():\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/torch/lib/python3.12/site-packages/torch/export/_trace.py:2037\u001b[39m, in \u001b[36m_export_for_training\u001b[39m\u001b[34m(mod, args, kwargs, dynamic_shapes, strict, preserve_module_call_signature)\u001b[39m\n\u001b[32m   2034\u001b[39m \u001b[38;5;66;03m# Call the appropriate export function based on the strictness of tracing.\u001b[39;00m\n\u001b[32m   2035\u001b[39m export_func = _strict_export \u001b[38;5;28;01mif\u001b[39;00m strict \u001b[38;5;28;01melse\u001b[39;00m _non_strict_export\n\u001b[32m-> \u001b[39m\u001b[32m2037\u001b[39m export_artifact = \u001b[43mexport_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2038\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2039\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2040\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2041\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdynamic_shapes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdynamic_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2042\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2043\u001b[39m \u001b[43m    \u001b[49m\u001b[43morig_in_spec\u001b[49m\u001b[43m=\u001b[49m\u001b[43morig_in_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2044\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_complex_guards_as_runtime_asserts\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2045\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_is_torch_jit_trace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2046\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_to_aten_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_export_to_aten_ir_make_fx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2047\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2049\u001b[39m export_graph_signature = export_artifact.aten.sig\n\u001b[32m   2051\u001b[39m forward_arg_names = _get_forward_arg_names(mod, args, kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/torch/lib/python3.12/site-packages/torch/export/_trace.py:1979\u001b[39m, in \u001b[36m_non_strict_export\u001b[39m\u001b[34m(mod, args, kwargs, dynamic_shapes, preserve_module_call_signature, orig_in_spec, allow_complex_guards_as_runtime_asserts, _is_torch_jit_trace, _to_aten_func)\u001b[39m\n\u001b[32m   1962\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[32m   1963\u001b[39m     fake_mode,\n\u001b[32m   1964\u001b[39m     _NonStrictTorchFunctionHandler(),\n\u001b[32m   1965\u001b[39m     tracing(tx),\n\u001b[32m   1966\u001b[39m     torch._dynamo.config.patch(dynamo_config),\n\u001b[32m   1967\u001b[39m ):\n\u001b[32m   1968\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[32m   1969\u001b[39m         _fakify_script_objects(mod, fake_args, fake_kwargs, fake_mode) \u001b[38;5;28;01mas\u001b[39;00m (\n\u001b[32m   1970\u001b[39m             patched_mod,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1977\u001b[39m         _override_builtin_ops(),\n\u001b[32m   1978\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1979\u001b[39m         aten_export_artifact = \u001b[43m_to_aten_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[operator]\u001b[39;49;00m\n\u001b[32m   1980\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpatched_mod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1981\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnew_fake_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1982\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnew_fake_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1983\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfake_params_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1984\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnew_fake_constant_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1985\u001b[39m \u001b[43m            \u001b[49m\u001b[43mproduce_guards_callback\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_produce_guards_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1986\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_tuplify_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1987\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1988\u001b[39m         \u001b[38;5;66;03m# aten_export_artifact.constants contains only fake script objects, we need to map them back\u001b[39;00m\n\u001b[32m   1989\u001b[39m         aten_export_artifact.constants = {\n\u001b[32m   1990\u001b[39m             fqn: map_fake_to_real[obj] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, FakeScriptObject) \u001b[38;5;28;01melse\u001b[39;00m obj\n\u001b[32m   1991\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m fqn, obj \u001b[38;5;129;01min\u001b[39;00m aten_export_artifact.constants.items()\n\u001b[32m   1992\u001b[39m         }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/torch/lib/python3.12/site-packages/torch/export/_trace.py:1770\u001b[39m, in \u001b[36m_export_to_aten_ir_make_fx\u001b[39m\u001b[34m(mod, fake_args, fake_kwargs, fake_params_buffers, constant_attrs, produce_guards_callback, transform)\u001b[39m\n\u001b[32m   1756\u001b[39m \u001b[38;5;66;03m# This _reparametrize_module makes sure inputs and module.params/buffers have the same fake_mode,\u001b[39;00m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# otherwise aot_export_module will error out because it sees a mix of fake_modes.\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# And we want aot_export_module to use the fake_tensor mode in dynamo to keep the pipeline easy to reason about.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[32m   1760\u001b[39m     torch.nn.utils.stateless._reparametrize_module(\n\u001b[32m   1761\u001b[39m         mod,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1768\u001b[39m     _compiling_state_context(),\n\u001b[32m   1769\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1770\u001b[39m     gm, graph_signature = \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_make_fx_helper\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1771\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1772\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfake_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1773\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrace_joint\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1774\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfake_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1775\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1777\u001b[39m     \u001b[38;5;66;03m# [NOTE] In training IR, we don't run\u001b[39;00m\n\u001b[32m   1778\u001b[39m     \u001b[38;5;66;03m# any DCE as a result we preserve constant\u001b[39;00m\n\u001b[32m   1779\u001b[39m     \u001b[38;5;66;03m# nodes in the graph. make_fx invariant is that\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1782\u001b[39m     \u001b[38;5;66;03m# graph, the node.meta here actually doesn't matter. But\u001b[39;00m\n\u001b[32m   1783\u001b[39m     \u001b[38;5;66;03m# we do this to make spec verifier happy.\u001b[39;00m\n\u001b[32m   1784\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m gm.graph.nodes:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/torch/lib/python3.12/site-packages/torch/export/_trace.py:1900\u001b[39m, in \u001b[36m_non_strict_export.<locals>._tuplify_outputs.<locals>._aot_export_non_strict\u001b[39m\u001b[34m(mod, args, kwargs, **flags)\u001b[39m\n\u001b[32m   1896\u001b[39m     ctx = _wrap_submodules(  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[32m   1897\u001b[39m         wrapped_mod, new_preserved_call_signatures, module_call_specs\n\u001b[32m   1898\u001b[39m     )\n\u001b[32m   1899\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ctx:\n\u001b[32m-> \u001b[39m\u001b[32m1900\u001b[39m     gm, sig = \u001b[43maot_export\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapped_mod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1901\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mExported program from AOTAutograd:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, gm)\n\u001b[32m   1903\u001b[39m sig.parameters = pytree.tree_map(_strip_root, sig.parameters)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/torch/lib/python3.12/site-packages/torch/export/_trace.py:1685\u001b[39m, in \u001b[36m_export_to_aten_ir_make_fx.<locals>._make_fx_helper\u001b[39m\u001b[34m(mod, args, kwargs, **flags)\u001b[39m\n\u001b[32m   1682\u001b[39m             k.\u001b[34m__getattribute__\u001b[39m = old_getattr  \u001b[38;5;66;03m# type: ignore[method-assign, attr-defined]\u001b[39;00m\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ctx, override_getattribute_for_subclasses(flat_args):\n\u001b[32m-> \u001b[39m\u001b[32m1685\u001b[39m     gm = \u001b[43mmake_fx\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1686\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwrapped_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1687\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrecord_module_stack\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1688\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m non_strict_root \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1692\u001b[39m     input_names = _graph_input_names(gm)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/torch/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py:2318\u001b[39m, in \u001b[36mmake_fx.<locals>.wrapped\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m   2316\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(f)\n\u001b[32m   2317\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(*args: \u001b[38;5;28mobject\u001b[39m) -> GraphModule:\n\u001b[32m-> \u001b[39m\u001b[32m2318\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmake_fx_tracer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/torch/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py:2250\u001b[39m, in \u001b[36m_MakefxTracer.trace\u001b[39m\u001b[34m(self, f, *args)\u001b[39m\n\u001b[32m   2248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrace\u001b[39m(\u001b[38;5;28mself\u001b[39m, f: Callable, *args: \u001b[38;5;28mobject\u001b[39m) -> fx.GraphModule:\n\u001b[32m   2249\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._init_modes_from_inputs(f, args):\n\u001b[32m-> \u001b[39m\u001b[32m2250\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_trace_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/torch/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py:2221\u001b[39m, in \u001b[36m_MakefxTracer._trace_inner\u001b[39m\u001b[34m(self, f, *args)\u001b[39m\n\u001b[32m   2219\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fx_tracer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2220\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2221\u001b[39m     t = \u001b[43mdispatch_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwrap_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfx_tracer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2223\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtracer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfx_tracer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2224\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mphs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2225\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2226\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   2227\u001b[39m     trace_structured(\n\u001b[32m   2228\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33martifact\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2229\u001b[39m         metadata_fn=\u001b[38;5;28;01mlambda\u001b[39;00m: {\n\u001b[32m   (...)\u001b[39m\u001b[32m   2238\u001b[39m         ).src,\n\u001b[32m   2239\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/torch/lib/python3.12/site-packages/torch/_compile.py:53\u001b[39m, in \u001b[36m_disable_dynamo.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     50\u001b[39m     disable_fn = torch._dynamo.disable(fn, recursive, wrapping=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     51\u001b[39m     fn.__dynamo_disable = disable_fn  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/torch/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    927\u001b[39m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m.callback))\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m929\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    931\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/torch/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py:1254\u001b[39m, in \u001b[36mdispatch_trace\u001b[39m\u001b[34m(root, tracer, concrete_args)\u001b[39m\n\u001b[32m   1248\u001b[39m \u001b[38;5;129m@torch\u001b[39m._disable_dynamo\n\u001b[32m   1249\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdispatch_trace\u001b[39m(\n\u001b[32m   1250\u001b[39m     root: Union[Module, Callable],\n\u001b[32m   1251\u001b[39m     tracer: Tracer,\n\u001b[32m   1252\u001b[39m     concrete_args: Optional[\u001b[38;5;28mtuple\u001b[39m[Any, ...]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1253\u001b[39m ) -> GraphModule:\n\u001b[32m-> \u001b[39m\u001b[32m1254\u001b[39m     graph = \u001b[43mtracer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m   1256\u001b[39m     \u001b[38;5;66;03m# NB: be careful not to DCE .item() calls\u001b[39;00m\n\u001b[32m   1257\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mimpure_pred\u001b[39m(n: fx.Node) -> \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/torch/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py:1835\u001b[39m, in \u001b[36m_ModuleStackTracer.trace\u001b[39m\u001b[34m(self, root, concrete_args)\u001b[39m\n\u001b[32m   1832\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrace\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[32m   1833\u001b[39m     \u001b[38;5;28mself\u001b[39m, root: Union[Module, Callable], concrete_args: Optional[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mobject\u001b[39m]]\n\u001b[32m   1834\u001b[39m ) -> fx.Graph:\n\u001b[32m-> \u001b[39m\u001b[32m1835\u001b[39m     res = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1837\u001b[39m     \u001b[38;5;66;03m# Since we are making _AttrProxy mimic the original\u001b[39;00m\n\u001b[32m   1838\u001b[39m     \u001b[38;5;66;03m# submodule, when someone registers a module directly\u001b[39;00m\n\u001b[32m   1839\u001b[39m     \u001b[38;5;66;03m# to the tracer while tracing, the proxy object gets registered\u001b[39;00m\n\u001b[32m   1840\u001b[39m     \u001b[38;5;66;03m# first. So we need to replace the proxy modules with the real ones\u001b[39;00m\n\u001b[32m   1841\u001b[39m     \u001b[38;5;66;03m# This can happen during HOO tracing\u001b[39;00m\n\u001b[32m   1842\u001b[39m     proxy_module_names_to_be_replaced: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, _AttrProxy]] = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/torch/lib/python3.12/site-packages/torch/fx/_symbolic_trace.py:850\u001b[39m, in \u001b[36mTracer.trace\u001b[39m\u001b[34m(self, root, concrete_args)\u001b[39m\n\u001b[32m    843\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._autowrap_search:\n\u001b[32m    844\u001b[39m             _autowrap_check(\n\u001b[32m    845\u001b[39m                 patcher, module.\u001b[34m__dict__\u001b[39m, \u001b[38;5;28mself\u001b[39m._autowrap_function_ids\n\u001b[32m    846\u001b[39m             )\n\u001b[32m    847\u001b[39m         \u001b[38;5;28mself\u001b[39m.create_node(\n\u001b[32m    848\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    849\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m850\u001b[39m             (\u001b[38;5;28mself\u001b[39m.create_arg(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m),),\n\u001b[32m    851\u001b[39m             {},\n\u001b[32m    852\u001b[39m             type_expr=fn.\u001b[34m__annotations__\u001b[39m.get(\u001b[33m\"\u001b[39m\u001b[33mreturn\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    853\u001b[39m         )\n\u001b[32m    855\u001b[39m     \u001b[38;5;28mself\u001b[39m.submodule_paths = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    856\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/torch/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py:1312\u001b[39m, in \u001b[36mwrap_key.<locals>.wrapped\u001b[39m\u001b[34m(*proxies, **_unused)\u001b[39m\n\u001b[32m   1309\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_tensor_proxy_slot\u001b[39m(t: Tensor) -> Union[Tensor, Proxy]:\n\u001b[32m   1310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m get_proxy_slot(t, tracer, t, \u001b[38;5;28;01mlambda\u001b[39;00m x: x.proxy)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1312\u001b[39m out = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type:ignore[call-arg]\u001b[39;00m\n\u001b[32m   1313\u001b[39m out = pytree.tree_map_only(Tensor, get_tensor_proxy_slot, out)\n\u001b[32m   1314\u001b[39m out = pytree.tree_map_only(\n\u001b[32m   1315\u001b[39m     _AnyScriptObject, \u001b[38;5;28;01mlambda\u001b[39;00m t: get_proxy_slot(t, tracer, t, \u001b[38;5;28;01mlambda\u001b[39;00m x: x), out\n\u001b[32m   1316\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:1\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(arg0, arg1, arg2, arg3, arg4, arg5, arg6, arg7, arg8, arg9, arg10, arg11, arg12, arg13, arg14, arg15, arg16, arg17, arg18, arg19, arg20, arg21, arg22, arg23, arg24, arg25, arg26, arg27, arg28, arg29, arg30, arg31, arg32, arg33, arg34, arg35, arg36, arg37, arg38, arg39, arg40, arg41, arg42, arg43, arg44, arg45, arg46, arg47, arg48, arg49, arg50, arg51, arg52, arg53, arg54, arg55, arg56, arg57, arg58, arg59, arg60, arg61, arg62, arg63, arg64, arg65, arg66, arg67, arg68, arg69, arg70, arg71, arg72, arg73, arg74, arg75, arg76, arg77, arg78, arg79, arg80, arg81, arg82, arg83, arg84, arg85, arg86, arg87, arg88, arg89, arg90, arg91, arg92, arg93, arg94, arg95, arg96, arg97, arg98, arg99, arg100, arg101, arg102, arg103, arg104, arg105, arg106, arg107, arg108, arg109, arg110, arg111, arg112, arg113, arg114, arg115, arg116, arg117, arg118, arg119, arg120, arg121, arg122, arg123, arg124, arg125, arg126, arg127, arg128, arg129, arg130, arg131, arg132, arg133, arg134, arg135, arg136, arg137, arg138, arg139, arg140, arg141, arg142, arg143, arg144, arg145, arg146, arg147, arg148, arg149, arg150, arg151, arg152, arg153, arg154, arg155, arg156, arg157, arg158, arg159, arg160, arg161, arg162, arg163, arg164, arg165, arg166, arg167, arg168, arg169, arg170)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/torch/lib/python3.12/site-packages/torch/export/_trace.py:1589\u001b[39m, in \u001b[36m_export_to_aten_ir_make_fx.<locals>._make_fx_helper.<locals>.wrapped_fn\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m   1587\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(flat_fn)\n\u001b[32m   1588\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped_fn\u001b[39m(*args):\n\u001b[32m-> \u001b[39m\u001b[32m1589\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[43mflat_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/torch/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py:193\u001b[39m, in \u001b[36mcreate_tree_flattened_fn.<locals>.flat_fn\u001b[39m\u001b[34m(*flat_args)\u001b[39m\n\u001b[32m    191\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    192\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_known_type:\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    194\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(i)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in output, which is not a known type. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    195\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mIf this type holds tensors, you need to register a pytree for it. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    196\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mSee https://github.com/pytorch/functorch/issues/475 for a brief \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    197\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mexplanation why. If you don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt need to register a pytree, please \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    198\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mleave a comment explaining your use case and we\u001b[39m\u001b[33m'\u001b[39m\u001b[33mll make this more \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    199\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mergonomic to deal with\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    200\u001b[39m         )\n\u001b[32m    201\u001b[39m out_spec.set(spec)\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m flat_out\n",
      "\u001b[31mRuntimeError\u001b[39m: Found <class 'transformers.cache_utils.EncoderDecoderCache'> in output, which is not a known type. If this type holds tensors, you need to register a pytree for it. See https://github.com/pytorch/functorch/issues/475 for a brief explanation why. If you don't need to register a pytree, please leave a comment explaining your use case and we'll make this more ergonomic to deal with"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "\n",
    "# load model\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n",
    "\n",
    "# dummy inputs for exporting the model\n",
    "input_features = torch.randn(1,80, 3000)\n",
    "attention_mask = torch.ones(1, 3000)\n",
    "decoder_input_ids = torch.tensor([[1, 1, 1 , 1]]) * model.config.decoder_start_token_id\n",
    "\n",
    "model.eval()\n",
    "\n",
    "exported_program: torch.export.ExportedProgram= torch.export.export(model, args=(input_features, attention_mask, decoder_input_ids,),\n",
    "                                                                    strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06451fb-c6f9-4731-b9f5-abbf1bef29a6",
   "metadata": {},
   "source": [
    "## Image Captioning\n",
    "Image Captioning is the task of defining the contents of an image in words. In the context of gaming, Image Captioning can be used to enhance the gameplay experience by dynamically generating text description of the various game objects in the scene, thereby providing the gamer with additional details. BLIP is a popular model for Image Captioning released by SalesForce Research. The code below tries to export BLIP with batch_size=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb0bf251-cbba-4152-ba31-e6fde2746822",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mblip\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m blip_decoder\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m#if torch.backends.mps.is_available() and torch.backends.mps.is_built():\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m#    device = torch.device(\"mps\")\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m#else:\u001b[39;00m\n\u001b[32m      8\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from models.blip import blip_decoder\n",
    "\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "image_size = 384\n",
    "image = torch.randn(1, 3,384,384).to(device)\n",
    "caption_input = \"\"\n",
    "\n",
    "model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth'\n",
    "model = blip_decoder(pretrained=model_url, image_size=image_size, vit='base')\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "exported_program: torch.export.ExportedProgram= torch.export.export(model, args=(image,caption_input,), strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0accb8e7-d0ba-424a-a49d-352488141aef",
   "metadata": {},
   "source": [
    "### Error: Cannot mutate tensors with frozen storage\n",
    "While exporting a model, it might fail because the model implementation might contain certain Python operations which are not yet supported by torch.export. Some of these failures may have a workaround. BLIP is an example where the original model errors, which can be resolved by making a small change in the code. torch.export lists the common cases of supported and unsupported operations in ExportDB and shows how you can modify your code to make it export compatible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4aeaf6c7-4454-4c92-8d67-67227aa52906",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<string>, line 3)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mFile \u001b[39m\u001b[32m<string>:3\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mFile \"/anaconda3/envs/export/lib/python3.10/site-packages/torch/_subclasses/functional_tensor.py\", line 545, in __torch_dispatch__\u001b[39m\n                                                                                                                                      ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "File \"/BLIP/models/blip.py\", line 112, in forward\n",
    "    text.input_ids[:,0] = self.tokenizer.bos_token_id\n",
    "  File \"/anaconda3/envs/export/lib/python3.10/site-packages/torch/_subclasses/functional_tensor.py\", line 545, in __torch_dispatch__\n",
    "    outs_unwrapped = func._op_dk(\n",
    "RuntimeError: cannot mutate tensors with frozen storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e45065-5897-4558-8bcd-212e36b95052",
   "metadata": {},
   "source": [
    "### Solution\n",
    "Clone the tensor where export fails.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f44bf67c-cc29-443b-95ef-c3c3c03f7884",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m text.input_ids = \u001b[43mtext\u001b[49m.input_ids.clone() \u001b[38;5;66;03m# clone the tensor\u001b[39;00m\n\u001b[32m      2\u001b[39m text.input_ids[:,\u001b[32m0\u001b[39m] = \u001b[38;5;28mself\u001b[39m.tokenizer.bos_token_id\n",
      "\u001b[31mNameError\u001b[39m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "text.input_ids = text.input_ids.clone() # clone the tensor\n",
    "text.input_ids[:,0] = self.tokenizer.bos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e906537-3fe4-4cd7-b170-7f1265c91b8e",
   "metadata": {},
   "source": [
    "## Promptable Image Segmentation\n",
    "Image segmentation is a computer vision technique that divides a digital image into distinct groups of pixels, or segments, based on their characteristics. Segment Anything Model (SAM)) introduced promptable image segmentation, which predicts object masks given prompts that indicate the desired object. SAM 2 is the first unified model for segmenting objects across images and videos. The SAM2ImagePredictor class provides an easy interface to the model for prompting the model. The model can take as input both point and box prompts, as well as masks from the previous iteration of prediction. Since SAM2 provides strong zero-shot performance for object tracking, it can be used for tracking game objects in a scene.\n",
    "\n",
    "The tensor operations in the predict method of SAM2ImagePredictor are happening in the _predict method. So, we try to export like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c6e522b-31ec-48c6-a708-db1b59cc756c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m ep = torch.export.export(\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m._predict,\n\u001b[32m      3\u001b[39m     args=(unnorm_coords, labels, unnorm_box, mask_input, multimask_output),\n\u001b[32m      4\u001b[39m     kwargs={\u001b[33m\"\u001b[39m\u001b[33mreturn_logits\u001b[39m\u001b[33m\"\u001b[39m: return_logits},\n\u001b[32m      5\u001b[39m     strict=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m      6\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "ep = torch.export.export(\n",
    "    self._predict,\n",
    "    args=(unnorm_coords, labels, unnorm_box, mask_input, multimask_output),\n",
    "    kwargs={\"return_logits\": return_logits},\n",
    "    strict=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b60e09-7eae-4406-ac2b-3e9117cdd73c",
   "metadata": {},
   "source": [
    "### Error: Model is not of type torch.nn.Module\n",
    "torch.export expects the module to be of type torch.nn.Module. However, the module we are trying to export is a class method. Hence it errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c961f5a5-5b29-47bc-9549-20820718c388",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (2434757216.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mTraceback (most recent call last):\u001b[39m\n               ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "Traceback (most recent call last):\n",
    "  File \"/sam2/image_predict.py\", line 20, in <module>\n",
    "    masks, scores, _ = predictor.predict(\n",
    "  File \"/sam2/sam2/sam2_image_predictor.py\", line 312, in predict\n",
    "    ep = torch.export.export(\n",
    "  File \"python3.10/site-packages/torch/export/__init__.py\", line 359, in export\n",
    "    raise ValueError(\n",
    "ValueError: Expected `mod` to be an instance of `torch.nn.Module`, got <class 'method'>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc6047e-c6c1-42d6-aa04-0bd0984720e2",
   "metadata": {},
   "source": [
    " ### Solution\n",
    "We write a helper class, which inherits from torch.nn.Module and call the _predict method in the forward method of the class. The complete code can be found here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "39b51aa3-364e-4221-b891-b4440e735fa5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unnorm_coords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      6\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._predict(*args, **kwargs)\n\u001b[32m      8\u001b[39m model_to_export = ExportHelper()\n\u001b[32m      9\u001b[39m ep = torch.export.export(\n\u001b[32m     10\u001b[39m       model_to_export,\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m       args=(\u001b[43munnorm_coords\u001b[49m, labels, unnorm_box, mask_input,  multimask_output),\n\u001b[32m     12\u001b[39m       kwargs={\u001b[33m\"\u001b[39m\u001b[33mreturn_logits\u001b[39m\u001b[33m\"\u001b[39m: return_logits},\n\u001b[32m     13\u001b[39m       strict=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     14\u001b[39m       )\n",
      "\u001b[31mNameError\u001b[39m: name 'unnorm_coords' is not defined"
     ]
    }
   ],
   "source": [
    "class ExportHelper(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(_, *args, **kwargs):\n",
    "        return self._predict(*args, **kwargs)\n",
    "\n",
    "model_to_export = ExportHelper()\n",
    "ep = torch.export.export(\n",
    "      model_to_export,\n",
    "      args=(unnorm_coords, labels, unnorm_box, mask_input,  multimask_output),\n",
    "      kwargs={\"return_logits\": return_logits},\n",
    "      strict=False,\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac7ef2e-c99f-4285-aef9-31f347fbcc4e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this tutorial, we have learned how to use torch.export to export models for popular use cases by addressing challenges through correct configuration and simple code modifications. Once you are able to export a model, you can lower the ExportedProgram into your hardware using AOTInductor in case of servers and ExecuTorch in case of edge device. To learn more about AOTInductor (AOTI), please refer to the AOTI tutorial. To learn more about ExecuTorch , please refer to the ExecuTorch tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bddc882-eb7c-4a45-8229-341010691c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
