{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b39cad01-1996-4e0d-92c3-1624de17b779",
   "metadata": {},
   "source": [
    "# quick triton breakdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccae548-7aa4-4a10-bfc4-838e8bb95827",
   "metadata": {},
   "source": [
    "## What You Should Know About Triton (Basics)\n",
    "\n",
    "### What is Triton?\n",
    "\n",
    "Triton is a language + compiler for writing custom GPU kernels. It lets you write high‑performance GPU code (e.g. vector ops, matrix ops) in a more Pythonic / domain-specific way, instead of writing raw CUDA.\n",
    "\n",
    "It was developed by OpenAI and is integrated in parts of PyTorch to allow fusion, custom kernels, etc.\n",
    "\n",
    "### Device / Backend Target\n",
    "\n",
    "Triton currently targets CUDA / NVIDIA GPUs. It generates code for GPU execution via CUDA.\n",
    "\n",
    "It is not compatible with MPS (Apple GPU) or other non‑CUDA backends (as of now), so it only works where CUDA is available.\n",
    "\n",
    "### triton.jit and Kernel Definitions\n",
    "\n",
    "You write kernels decorated with @triton.jit, defining how memory is loaded, processed, stored, etc.\n",
    "\n",
    "The kernel expresses parallelism (e.g. divide input array among program IDs) and handles masking, vectorization, etc.\n",
    "\n",
    "### Kernel Invocation / Launching\n",
    "\n",
    "You call kernels with special syntax: kernel[grid](...) where grid is a function of meta parameters and the input tensor sizes.\n",
    "\n",
    "The grid determines how many blocks / threads you dispatch.\n",
    "\n",
    "### Autotuning / Heuristics\n",
    "\n",
    "Triton supports autotuning, to explore different kernel configurations (block sizes, warps, stages) to find the best performance per hardware.\n",
    "\n",
    "You can specify @triton.autotune(...) around your kernel(s) and pass multiple config choices.\n",
    "\n",
    "PyTorch’s torch.compile (when integrated) must respect those tunable configurations.\n",
    "\n",
    "### Integration with PyTorch\n",
    "\n",
    "You can embed Triton kernels inside PyTorch code and have torch.compile optimize across kernels + surrounding PyTorch code.\n",
    "\n",
    "But to fully integrate (autograd, fallback, composability with PyTorch features), you often wrap Triton kernels into torch.library.triton_op or use torch.library mechanisms.\n",
    "\n",
    "### Limitations & Fallbacks\n",
    "\n",
    "Triton kernels only run on GPU (CUDA). They do not run on CPU automatically. You may need to provide a CPU fallback.\n",
    "\n",
    "Not all PyTorch subsystems (like tensor subclasses, custom autograd, flop counting, etc.) are automatically compatible unless wrapped properly.\n",
    "\n",
    "Some Triton features (e.g. heuristics vs autotune order) have constraints when used with torch.compile.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fc7030-8c10-4260-9926-8b8f47c2d347",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
