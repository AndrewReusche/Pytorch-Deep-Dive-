{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bdef5db-9bb7-492b-88ba-b772e61be182",
   "metadata": {},
   "source": [
    "# Compile Time Caching in torch.compile\n",
    "\n",
    "## Introduction\n",
    "PyTorch Compiler provides several caching offerings to reduce compilation latency. This recipe will explain these offerings in detail to help users pick the best option for their use case.\n",
    "\n",
    "Check out Compile Time Caching Configurations for how to configure these caches.\n",
    "\n",
    "Also check out our caching benchmark at PT CacheBench Benchmarks.\n",
    "\n",
    "## Prerequisites\n",
    "Before starting this recipe, make sure that you have the following:\n",
    "\n",
    "Basic understanding of torch.compile. See:\n",
    "\n",
    "torch.compiler API documentation\n",
    "\n",
    "Introduction to torch.compile\n",
    "\n",
    "Triton language documentation\n",
    "\n",
    "PyTorch 2.4 or later\n",
    "\n",
    "## Caching Offerings\n",
    "torch.compile provides the following caching offerings:\n",
    "\n",
    "End to end caching (also known as Mega-Cache)\n",
    "\n",
    "Modular caching of TorchDynamo, TorchInductor, and Triton\n",
    "\n",
    "It is important to note that caching validates that the cache artifacts are used with the same PyTorch and Triton version, as well as, same GPU when device is set to be cuda.\n",
    "\n",
    "## torch.compile end-to-end caching (Mega-Cache)\n",
    "End to end caching, from here onwards referred to Mega-Cache, is the ideal solution for users looking for a portable caching solution that can be stored in a database and can later be fetched possibly on a separate machine.\n",
    "\n",
    "Mega-Cache provides two compiler APIs:\n",
    "\n",
    "torch.compiler.save_cache_artifacts()\n",
    "\n",
    "torch.compiler.load_cache_artifacts()\n",
    "\n",
    "The intended use case is after compiling and executing a model, the user calls torch.compiler.save_cache_artifacts() which will return the compiler artifacts in a portable form. Later, potentially on a different machine, the user may call torch.compiler.load_cache_artifacts() with these artifacts to pre-populate the torch.compile caches in order to jump-start their cache.\n",
    "\n",
    "Consider the following example. First, compile and save the cache artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07e050fe-03bc-455b-b9cc-ea79de320e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "device='cpu' #example\n",
    "dtype = torch.float32 #example\n",
    "\n",
    "@torch.compile\n",
    "def fn(x, y):\n",
    "    return x.sin() @ y\n",
    "\n",
    "a = torch.rand(100, 100, dtype=dtype, device=device)\n",
    "b = torch.rand(100, 100, dtype=dtype, device=device)\n",
    "\n",
    "result = fn(a, b)\n",
    "\n",
    "artifacts = torch.compiler.save_cache_artifacts()\n",
    "\n",
    "assert artifacts is not None\n",
    "artifact_bytes, cache_info = artifacts\n",
    "\n",
    "# Now, potentially store artifact_bytes in a database\n",
    "# You can use cache_info for logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1a3659-e67d-49a2-984b-6f5b781eb2b5",
   "metadata": {},
   "source": [
    "\n",
    "Later, you can jump-start the cache by the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bcc43af-dcd6-4efc-8f2c-fd9ef4418843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CacheInfo(artifacts=defaultdict(<class 'list'>, {'inductor': ['f44daqjreg3zkozij2zpqqrz73qr7rdj3qyivsw2nk55c3jngsdc'], 'aot_autograd': ['aaybbapudsllg7qmno4kzouku7qfreyxzvsd32lnsyvfpkl3ya5a']}))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Potentially download/fetch the artifacts from the database\n",
    "torch.compiler.load_cache_artifacts(artifact_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e763a168-e3f2-440f-976a-8b3d5e43a344",
   "metadata": {},
   "source": [
    "This operation populates all the modular caches that will be discussed in the next section, including PGO, AOTAutograd, Inductor, Triton, and Autotuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e965fe-db41-4f94-a03e-8503a70de9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
