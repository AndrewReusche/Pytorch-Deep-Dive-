{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f60266b6-f792-4537-8d28-7cfcc9aa00ec",
   "metadata": {},
   "source": [
    "# Running the compiled optimizer with an LR Scheduler\n",
    "\n",
    "The optimizer is a key algorithm for training any deep learning model. In this example, we will show how to pair the optimizer, which has been compiled using torch.compile, with the LR schedulers to accelerate training convergence.\n",
    "\n",
    "Note\n",
    "\n",
    "This tutorial requires PyTorch 2.3.0 or later.\n",
    "\n",
    "## Model Setup\n",
    "For this example, we’ll use a simple sequence of linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5f4a11f-643b-49d8-867b-b69ea0e1e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#since on mac switch cuda for mps\n",
    "# Create simple model\n",
    "model = torch.nn.Sequential(\n",
    "    *[torch.nn.Linear(1024, 1024, False, device=\"mps\") for _ in range(10)]\n",
    ")\n",
    "input = torch.rand(1024, device=\"mps\")\n",
    "\n",
    "# run forward pass\n",
    "output = model(input)\n",
    "\n",
    "# run backward to populate the grads for our optimizer below\n",
    "output.sum().backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aa4580-aa3e-440a-9d2b-5a5ea266e035",
   "metadata": {},
   "source": [
    "## Setting up and running the compiled optimizer with LR Scheduler\n",
    "In this section, we’ll use the Adam optimizer with LinearLR Scheduler and create a helper function to wrap the step() call for each of them in torch.compile().\n",
    "\n",
    "Note\n",
    "\n",
    "torch.compile is only supported on CUDA devices that have a compute capability of 7.0 or higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69beabed-513e-479c-ad94-45384c623fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MPS is available and can be used.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1013 19:19:21.051000 43751 site-packages/torch/_logging/_internal.py:1154] [1/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0047)\n",
      "tensor(0.0060)\n",
      "tensor(0.0073)\n",
      "tensor(0.0087)\n",
      "tensor(0.0100)\n"
     ]
    }
   ],
   "source": [
    "#not relevant since on mac\n",
    "# exit cleanly if we are on a device that doesn't support ``torch.compile``\n",
    "#if torch.cuda.get_device_capability() < (7, 0):\n",
    "#    print(\"Exiting because torch.compile is not supported on this device.\")\n",
    "#    import sys\n",
    "#    sys.exit(0)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"✅ MPS is available and can be used.\")\n",
    "else:\n",
    "    print(\"❌ MPS is not available.\")\n",
    "    \n",
    "# !!! IMPORTANT !!! Wrap the lr in a Tensor if we are pairing the\n",
    "# the optimizer with an LR Scheduler.\n",
    "# Without this, torch.compile will recompile as the value of the LR\n",
    "# changes.\n",
    "opt = torch.optim.Adam(model.parameters(), lr=torch.tensor(0.01))\n",
    "sched = torch.optim.lr_scheduler.LinearLR(opt, total_iters=5)\n",
    "\n",
    "@torch.compile(fullgraph=False)\n",
    "def fn():\n",
    "    opt.step()\n",
    "    sched.step()\n",
    "\n",
    "\n",
    "# Warmup runs to compile the function\n",
    "for _ in range(5):\n",
    "    fn()\n",
    "    print(opt.param_groups[0][\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1249b653-3811-45b5-b1f8-bf431e20b8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eager runtime: 6512.481238572828us\n",
      "compiled runtime: 59.544343926094584us\n"
     ]
    }
   ],
   "source": [
    "# Let's define a helpful benchmarking function:\n",
    "import torch.utils.benchmark as benchmark\n",
    "\n",
    "def benchmark_torch_function_in_microseconds(f, *args, **kwargs):\n",
    "    t0 = benchmark.Timer(\n",
    "        stmt=\"f(*args, **kwargs)\", globals={\"args\": args, \"kwargs\": kwargs, \"f\": f}\n",
    "    )\n",
    "    return t0.blocked_autorange().mean * 1e6\n",
    "\n",
    "eager_runtime = benchmark_torch_function_in_microseconds(opt.step)\n",
    "compiled_runtime = benchmark_torch_function_in_microseconds(fn)\n",
    "\n",
    "assert eager_runtime > compiled_runtime\n",
    "\n",
    "print(f\"eager runtime: {eager_runtime}us\")\n",
    "print(f\"compiled runtime: {compiled_runtime}us\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9cdf2b-5949-4002-985a-5bcaa63c6879",
   "metadata": {},
   "source": [
    "## Extension: What happens with a non-tensor LR?\n",
    "For the curious, we will show how to peek into what happens with torch.compile when we don’t wrap the LR in a tensor.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8286d70-6b0e-4bc9-ba15-aa413c8d504c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V1013 19:28:51.526000 43751 site-packages/torch/_dynamo/guards.py:3508] [1/2] [__recompiles] Recompiling function wrapper in /opt/anaconda3/envs/torch/lib/python3.12/site-packages/torch/optim/optimizer.py:496\n",
      "V1013 19:28:51.526000 43751 site-packages/torch/_dynamo/guards.py:3508] [1/2] [__recompiles]     triggered by the following guard failure(s):\n",
      "V1013 19:28:51.526000 43751 site-packages/torch/_dynamo/guards.py:3508] [1/2] [__recompiles]     - 1/1: Cache line invalidated because L['args'][0] got deallocated\n",
      "V1013 19:28:51.526000 43751 site-packages/torch/_dynamo/guards.py:3508] [1/2] [__recompiles]     - 1/0: Cache line invalidated because L['args'][0] got deallocated\n",
      "V1013 19:28:51.541000 43751 site-packages/torch/_dynamo/guards.py:3508] [2/2] [__recompiles] Recompiling function step in /opt/anaconda3/envs/torch/lib/python3.12/site-packages/torch/optim/adam.py:213\n",
      "V1013 19:28:51.541000 43751 site-packages/torch/_dynamo/guards.py:3508] [2/2] [__recompiles]     triggered by the following guard failure(s):\n",
      "V1013 19:28:51.541000 43751 site-packages/torch/_dynamo/guards.py:3508] [2/2] [__recompiles]     - 2/1: Cache line invalidated because L['self'] got deallocated\n",
      "V1013 19:28:51.541000 43751 site-packages/torch/_dynamo/guards.py:3508] [2/2] [__recompiles]     - 2/0: Cache line invalidated because L['self'] got deallocated\n"
     ]
    }
   ],
   "source": [
    "# No longer wrap the LR in a tensor here\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "sched = torch.optim.lr_scheduler.LinearLR(opt, total_iters=5)\n",
    "\n",
    "@torch.compile(fullgraph=False)\n",
    "def fn():\n",
    "    opt.step()\n",
    "    sched.step()\n",
    "\n",
    "# Setup logging to view recompiles\n",
    "torch._logging.set_logs(recompiles=True)\n",
    "\n",
    "# Warmup runs to compile the function\n",
    "# We will now recompile on each iteration\n",
    "# as the value of the lr is mutated.\n",
    "for _ in range(5):\n",
    "    fn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da84826e-00c3-4851-a245-576d9d412af9",
   "metadata": {},
   "source": [
    "With this example, we can see that we recompile the optimizer a few times due to the guard failure on the lr in param_groups[0].\n",
    "\n",
    "## Conclusion\n",
    "In this tutorial we showed how to pair the optimizer compiled with torch.compile with an LR Scheduler to accelerate training convergence. We used a model consisting of a simple sequence of linear layers with the Adam optimizer paired with a LinearLR scheduler to demonstrate the LR changing across iterations.\n",
    "\n",
    "See also:\n",
    "\n",
    "Compiled optimizer tutorial - an intro into the compiled optimizer.\n",
    "\n",
    "Compiling the optimizer with PT2 - deeper technical details on the compiled optimizer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd25f02c-7e25-48dc-b365-bf6521297bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
