{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bbefc3d-8606-4d1e-a420-4ff574ef538b",
   "metadata": {},
   "source": [
    "# Dynamic Compilation Control with torch.compiler.set_stance\n",
    "\n",
    "torch.compiler.set_stance is a torch.compiler API that enables you to change the behavior of torch.compile across different calls to your model without having to reapply torch.compile to your model.\n",
    "\n",
    "This recipe provides some examples on how to use torch.compiler.set_stance.\n",
    "\n",
    "# Prerequisites\n",
    "torch >= 2.6\n",
    "\n",
    "# Description\n",
    "torch.compile.set_stance can be used as a decorator, context manager, or raw function to change the behavior of torch.compile across different calls to your model.\n",
    "\n",
    "In the example below, the \"force_eager\" stance ignores all torch.compile directives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcac9b74-ef62-4b92-9571-f44b935254b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "@torch.compile\n",
    "def foo(x):\n",
    "    if torch.compiler.is_compiling():\n",
    "        # torch.compile is active\n",
    "        return x + 1\n",
    "    else:\n",
    "        # torch.compile is not active\n",
    "        return x - 1\n",
    "\n",
    "\n",
    "inp = torch.zeros(3)\n",
    "\n",
    "print(foo(inp))  # compiled, prints 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9d02f6-0741-4eb2-909e-5f464617b746",
   "metadata": {},
   "source": [
    "Sample decorator usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a095efea-7a53-410b-a1d2-5aafac10448d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1., -1., -1.])\n"
     ]
    }
   ],
   "source": [
    "@torch.compiler.set_stance(\"force_eager\")\n",
    "def bar(x):\n",
    "    # force disable the compiler\n",
    "    return foo(x)\n",
    "\n",
    "\n",
    "print(bar(inp))  # not compiled, prints -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8786dd-131c-4c75-9805-1d6887618b4d",
   "metadata": {},
   "source": [
    "Sample context manager usage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3e479bb-4c93-4b93-b3cc-ef93563159f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1., -1., -1.])\n"
     ]
    }
   ],
   "source": [
    "with torch.compiler.set_stance(\"force_eager\"):\n",
    "    print(foo(inp))  # not compiled, prints -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8ffa44-b3d7-4892-8576-070927550e4d",
   "metadata": {},
   "source": [
    "Sample raw function usage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9330cb00-f1ac-4a4b-9e86-3ae925dbd05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1., -1., -1.])\n",
      "tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "torch.compiler.set_stance(\"force_eager\")\n",
    "print(foo(inp))  # not compiled, prints -1\n",
    "torch.compiler.set_stance(\"default\")\n",
    "\n",
    "print(foo(inp))  # compiled, prints 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cefe22-da8e-4648-979d-66df080a6a63",
   "metadata": {},
   "source": [
    "torch.compile stance can only be changed outside of any torch.compile region. Attempts to do otherwise will result in an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92dfebce-b04b-46cc-94bc-966dd443d41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt to trace forbidden callable <function set_stance at 0x1294b2a20>\n",
      "\n",
      "from user code:\n",
      "   File \"/var/folders/p_/7g5qyc8d5zv5nfywm65yhpkc0000gn/T/ipykernel_90361/286248600.py\", line 4, in baz\n",
      "    with torch.compiler.set_stance(\"force_eager\"):\n",
      "\n",
      "Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n",
      "\n",
      "Attempt to trace forbidden callable <function inner at 0x13c229300>\n",
      "\n",
      "from user code:\n",
      "   File \"/var/folders/p_/7g5qyc8d5zv5nfywm65yhpkc0000gn/T/ipykernel_90361/286248600.py\", line 22, in outer\n",
      "    return inner(x)\n",
      "\n",
      "Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@torch.compile\n",
    "def baz(x):\n",
    "    # error!\n",
    "    with torch.compiler.set_stance(\"force_eager\"):\n",
    "        return x + 1\n",
    "\n",
    "\n",
    "try:\n",
    "    baz(inp)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "\n",
    "@torch.compiler.set_stance(\"force_eager\")\n",
    "def inner(x):\n",
    "    return x + 1\n",
    "\n",
    "\n",
    "@torch.compile\n",
    "def outer(x):\n",
    "    # error!\n",
    "    return inner(x)\n",
    "\n",
    "\n",
    "try:\n",
    "    outer(inp)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a39069-37d3-4dc7-98fa-98cd2443397d",
   "metadata": {},
   "source": [
    "Other stances include:\n",
    "    \n",
    "    \"default\": The default stance, used for normal compilation.\n",
    "\n",
    "    \"eager_on_recompile\": Run code eagerly when a recompile is necessary. If there is cached compiled code valid for the input, it will still be used.\n",
    "\n",
    "    \"fail_on_recompile\": Raise an error when recompiling a function.\n",
    "\n",
    "See the torch.compiler.set_stance doc page for more stances and options. More stances/options may also be added in the future.\n",
    "\n",
    "## Examples\n",
    "### Preventing recompilation\n",
    "Some models do not expect any recompilations - for example, you may always have inputs with the same shape. Since recompilations may be expensive, we may wish to error out when we attempt to recompile so we can detect and fix recompilation cases. The \"fail_on_recompilation\" stance can be used for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99878c58-1a6f-4dc3-bf49-af6588c5ff1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected recompile when torch.compile stance is 'fail_on_recompile'\n"
     ]
    }
   ],
   "source": [
    "@torch.compile\n",
    "def my_big_model(x):\n",
    "    return torch.relu(x)\n",
    "\n",
    "\n",
    "# first compilation\n",
    "my_big_model(torch.randn(3))\n",
    "\n",
    "with torch.compiler.set_stance(\"fail_on_recompile\"):\n",
    "    my_big_model(torch.randn(3))  # no recompilation - OK\n",
    "    try:\n",
    "        my_big_model(torch.randn(4))  # recompilation - error\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1570c185-9e43-4364-88a9-9b7a93147832",
   "metadata": {},
   "source": [
    "If erroring out is too disruptive, we can use \"eager_on_recompile\" instead, which will cause torch.compile to fall back to eager instead of erroring out. This may be useful if we don’t expect recompilations to happen frequently, but when one is required, we’d rather pay the cost of running eagerly over the cost of recompilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e67f503-57e3-4260-95ec-561fae05e7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.])\n",
      "tensor([1., 1., 1.])\n",
      "tensor([-1., -1., -1., -1.])\n",
      "tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "@torch.compile\n",
    "def my_huge_model(x):\n",
    "    if torch.compiler.is_compiling():\n",
    "        return x + 1\n",
    "    else:\n",
    "        return x - 1\n",
    "\n",
    "\n",
    "# first compilation\n",
    "print(my_huge_model(torch.zeros(3)))  # 1\n",
    "\n",
    "with torch.compiler.set_stance(\"eager_on_recompile\"):\n",
    "    print(my_huge_model(torch.zeros(3)))  # 1\n",
    "    print(my_huge_model(torch.zeros(4)))  # -1\n",
    "    print(my_huge_model(torch.zeros(3)))  # 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e884f2-0b57-446b-900d-14e1b8866835",
   "metadata": {},
   "source": [
    "## Measuring performance gains\n",
    "torch.compiler.set_stance can be used to compare eager vs. compiled performance without having to define a separate eager model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a30af4c-5874-4c17-8ef4-576ccae04b36",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tried to instantiate dummy base class Event",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m inps = torch.randn(\u001b[32m5\u001b[39m, \u001b[32m5\u001b[39m), torch.randn(\u001b[32m5\u001b[39m, \u001b[32m5\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.compiler.set_stance(\u001b[33m\"\u001b[39m\u001b[33mforce_eager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33meager:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mtimed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmy_gigantic_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minps\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m])\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# warmups\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m3\u001b[39m):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mtimed\u001b[39m\u001b[34m(fn)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtimed\u001b[39m(fn):\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     start = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEvent\u001b[49m\u001b[43m(\u001b[49m\u001b[43menable_timing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     end = torch.cuda.Event(enable_timing=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      7\u001b[39m     start.record()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/torch/lib/python3.12/site-packages/torch/cuda/streams.py:170\u001b[39m, in \u001b[36mEvent.__new__\u001b[39m\u001b[34m(cls, enable_timing, blocking, interprocess, external)\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__new__\u001b[39m(\n\u001b[32m    168\u001b[39m     \u001b[38;5;28mcls\u001b[39m, enable_timing=\u001b[38;5;28;01mFalse\u001b[39;00m, blocking=\u001b[38;5;28;01mFalse\u001b[39;00m, interprocess=\u001b[38;5;28;01mFalse\u001b[39;00m, external=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    169\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__new__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m        \u001b[49m\u001b[43menable_timing\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable_timing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mblocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43minterprocess\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterprocess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexternal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexternal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/torch/lib/python3.12/site-packages/torch/_utils.py:996\u001b[39m, in \u001b[36m_dummy_type.<locals>.get_err_fn.<locals>.err_fn\u001b[39m\u001b[34m(obj, *args, **kwargs)\u001b[39m\n\u001b[32m    994\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    995\u001b[39m     class_name = obj.\u001b[34m__name__\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m996\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTried to instantiate dummy base class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Tried to instantiate dummy base class Event"
     ]
    }
   ],
   "source": [
    "# Returns the result of running `fn()` and the time it took for `fn()` to run,\n",
    "# in seconds. We use CUDA events and synchronization for the most accurate\n",
    "# measurements.\n",
    "def timed(fn):\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    start.record()\n",
    "    result = fn()\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    return result, start.elapsed_time(end) / 1000\n",
    "\n",
    "\n",
    "@torch.compile\n",
    "def my_gigantic_model(x, y):\n",
    "    x = x @ y\n",
    "    x = x @ y\n",
    "    x = x @ y\n",
    "    return x\n",
    "\n",
    "\n",
    "inps = torch.randn(5, 5), torch.randn(5, 5)\n",
    "\n",
    "with torch.compiler.set_stance(\"force_eager\"):\n",
    "    print(\"eager:\", timed(lambda: my_gigantic_model(*inps))[1])\n",
    "\n",
    "# warmups\n",
    "for _ in range(3):\n",
    "    my_gigantic_model(*inps)\n",
    "\n",
    "print(\"compiled:\", timed(lambda: my_gigantic_model(*inps))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e94cc40-47bc-4b79-a9cd-2b44ef1fc70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eager: 0.01941394805908203\n",
      "compiled: 2.002716064453125e-05\n"
     ]
    }
   ],
   "source": [
    "# try with cpu timing \n",
    "\n",
    "import time\n",
    "\n",
    "def timed(fn):\n",
    "    start = time.time()\n",
    "    result = fn()\n",
    "    torch.mps.synchronize()  # if using MPS\n",
    "    return result, time.time() - start\n",
    "\n",
    "@torch.compile\n",
    "def my_gigantic_model(x, y):\n",
    "    x = x @ y\n",
    "    x = x @ y\n",
    "    x = x @ y\n",
    "    return x\n",
    "\n",
    "\n",
    "inps = torch.randn(5, 5), torch.randn(5, 5)\n",
    "\n",
    "with torch.compiler.set_stance(\"force_eager\"):\n",
    "    print(\"eager:\", timed(lambda: my_gigantic_model(*inps))[1])\n",
    "\n",
    "# warmups\n",
    "for _ in range(3):\n",
    "    my_gigantic_model(*inps)\n",
    "\n",
    "print(\"compiled:\", timed(lambda: my_gigantic_model(*inps))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4dca086-d8b4-4b3c-b98d-3d77b0e46aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1007 22:31:11.993000 90361 site-packages/torch/_inductor/utils.py:1436] [6/0] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eager: 0.47011804580688477\n",
      "compiled: 0.0009679794311523438\n"
     ]
    }
   ],
   "source": [
    "#try it with MPS aware timing\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "@torch.compile\n",
    "def my_gigantic_model(x, y):\n",
    "    x = x @ y\n",
    "    x = x @ y\n",
    "    x = x @ y\n",
    "    return x\n",
    "\n",
    "inps = torch.randn(5, 5, device=device), torch.randn(5, 5, device=device)\n",
    "\n",
    "# Timing with MPS sync\n",
    "def timed(fn):\n",
    "    import time\n",
    "    start = time.time()\n",
    "    result = fn()\n",
    "    if device.type == \"mps\":\n",
    "        torch.mps.synchronize()\n",
    "    return result, time.time() - start\n",
    "\n",
    "with torch.compiler.set_stance(\"force_eager\"):\n",
    "    print(\"eager:\", timed(lambda: my_gigantic_model(*inps))[1])\n",
    "\n",
    "# Warm-up\n",
    "for _ in range(3):\n",
    "    my_gigantic_model(*inps)\n",
    "\n",
    "print(\"compiled:\", timed(lambda: my_gigantic_model(*inps))[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdbbbff-fe87-4b96-9a6c-bdef5af6f6b8",
   "metadata": {},
   "source": [
    "# Crashing sooner\n",
    "Running an eager iteration first before a compiled iteration using the \"force_eager\" stance can help us to catch errors unrelated to torch.compile before attempting a very long compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "322a42db-6c3b-4c94-ae39-d720eebe5307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sin() takes 1 positional argument but 2 were given\n"
     ]
    }
   ],
   "source": [
    "@torch.compile\n",
    "def my_humongous_model(x):\n",
    "    return torch.sin(x, x)\n",
    "\n",
    "\n",
    "try:\n",
    "    with torch.compiler.set_stance(\"force_eager\"):\n",
    "        print(my_humongous_model(torch.randn(3)))\n",
    "    # this call to the compiled model won't run\n",
    "    print(my_humongous_model(torch.randn(3)))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cda78d-2ea9-4239-a7e9-ee526c7d99ed",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this recipe, we have learned how to use the torch.compiler.set_stance API to modify the behavior of torch.compile across different calls to a model without needing to reapply it. The recipe demonstrates using torch.compiler.set_stance as a decorator, context manager, or raw function to control compilation stances like force_eager, default, eager_on_recompile, and “fail_on_recompile.”\n",
    "\n",
    "For more information, see: torch.compiler.set_stance API documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142d9056-b47d-42da-baf0-0529480acc41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
